# This is a "production" config, intended to run on the real DRAO backend, 
# and receive data from the real correlator!
#
# See ch_frb_l1/OPERATIONS_MANAUL.md for usage.
#
# This is a 16-beam example.  Currently, in 16-beam configurations we are limited 
# to using "placeholder" RFI removal (detrenders but not clippers), and dedispersion
# using the least optimal settings (no spectral index search, no low-DM upsampled tree).
# This won't work on real data, since we don't remove RFI!
#
# In this example, we still use a "toy" version of the L1B code which just prints messages
# as it receives coarse-grained triggers.  (It it also supposed to make a trigger plot, but 
# this doesn't work in production yet, since the production L1 server has no way of shutting 
# down gracefully.)

nbeams: 16
nfreq: 16384
nt_per_packet: 16

beam_ids: [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]

# This example assumes the nodes are in a non-link-bonded configuration, where
# each of the four 1 Gbps NIC's is on an independent /24 network.  We use UDP
# port 1313 on all four NIC's.  Traffic is divided between NIC's "per beam", 
# i.e. four beams will be sent to each NIC.  All NIC's use TCP port 5555 for
# their RPC server.

ipaddr: [ "10.6.200.14",
	  "10.7.200.14",
	  "10.8.200.14",
	  "10.9.200.14" ]

port: 1313

rpc_address: [ "tcp://10.6.200.14:5555", 
	       "tcp://10.7.200.14:5555", 
	       "tcp://10.8.200.14:5555", 
	       "tcp://10.9.200.14:5555" ]

# The L1 node is configured so that it "thinks" the NFS server is four different
# servers, mounted on directories /frb-archiver1, /frb-archiver2, /frb-archiver3,
# /frb-archiver4.  File writes to each of these four filesystems will be sent from
# the corresponding NIC.  The node is responsible for load-balancing file writes 
# between these filesystems.
#
# Here, we define 5 output_devices, corresponding to (4 NIC's) + (1 SSD).
# Each output device will get a separate file I/O thread.  This allows file
# I/O on each device to proceed independently.

output_devices: [ "/local", "/frb-archiver-1", "/frb-archiver-2", "/frb-archiver-3", "/frb-archiver-4" ]

# Need to use fast kernels in this example (slow kernels are too slow.)
slow_kernels: False


# Buffer configuration.  For documentation on these parameters, see 
# "Config file reference: L1 server" in MANUAL.md.
#
# Note: with write_staging_area_gb=64, the L1 server will use 
# ~231 GB memory (the L1 node has 256 GB available).

assembled_ringbuf_nsamples: 10000
telescoping_ringbuf_nsamples: [ 30000, 60000, 60000 ]
write_staging_area_gb: 64.0


# L1b configuration.
#
# Postprocess triggers using 'toy-l1b.py', a placeholder version
# of the L1b code which "processes" coarse-grained triggers by
# making a big waterfall plot (toy_l1b_beam*.png).
#
# For a production-scale example, it makes sense to set l1b_pipe_timeout=0,
# and set l1b_buffer_nsamples based on the maximum acceptable latency between
# L1a and L1b (in this case we use 4000, corresponding to 4 seconds).  See
# MANUAL.md for discussion!

l1b_executable_filename: "./toy-l1b.py"
l1b_buffer_nsamples: 4000
l1b_pipe_timeout: 0


# stream_acqname.
#
# If the line below is uncommented, the node will continuously stream all incoming data
# to its local SSD.  The stream_acqname should be a unique identifying string which is
# not the same as any existing acquisition on the node.  The data will be written to
# /local/acq_data/$(stream_acqname).

# stream_acqname: "test_acq"
